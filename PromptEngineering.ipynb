{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a1a52b6",
   "metadata": {},
   "source": [
    "# Prompt Engineering on SageMaker Jumpstart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acea92d",
   "metadata": {},
   "source": [
    "---\n",
    "In this notebook we will use Amazon [SageMaker JumpStart](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html)to try out the various Prompt Engineering Techniques specified here [Prompt Engineering on SageMaker](https://www.promptingguide.ai/).\n",
    "\n",
    "\n",
    "In this demo notebook, we demonstrate how to use the SageMaker Python SDK for deploying Foundation Models as an endpoint and subsequently use them to test the Prompts we curate. The Foundation models perform **Text2Text Generation**. It takes a prompting text as an input, and returns the text generated by the model according to the prompt.\n",
    "\n",
    "Here, we show how to use the state-of-the-art pre-trained **FLAN T5 models** from [Hugging Face](https://huggingface.co/docs/transformers/model_doc/flan-t5) for Text2Text Generation in the following tasks. You can directly use FLAN-T5 model for many NLP tasks, without fine-tuning the model.\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815c0bc7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7e35194",
   "metadata": {},
   "source": [
    "Note: This notebook was tested on ml.t3.medium instance in Amazon SageMaker Studio with Python 3 (Data Science) kernel and in Amazon SageMaker Notebook instance with conda_python3 kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f8dfad",
   "metadata": {},
   "source": [
    "### 1. Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f31be0",
   "metadata": {},
   "source": [
    "---\n",
    "Before executing the notebook, there are some initial steps required for set up. This notebook requires ipywidgets.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb67d497",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker-datawrangler 0.4.3 requires sagemaker-data-insights==0.4.0, but you have sagemaker-data-insights 0.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.29.63 requires botocore==1.31.63, but you have botocore 1.33.12 which is incompatible.\n",
      "awscli 1.29.63 requires s3transfer<0.8.0,>=0.7.0, but you have s3transfer 0.8.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets==7.0.0 --quiet\n",
    "!pip install --upgrade sagemaker --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769f5d81",
   "metadata": {},
   "source": [
    "#### Permissions and environment variables\n",
    "\n",
    "---\n",
    "To host on Amazon SageMaker, we need to set up and authenticate the use of AWS services. Here, we use the execution role associated with the current notebook as the AWS account role with SageMaker access. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67131eee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69849d02",
   "metadata": {},
   "source": [
    "## 2. Select a pre-trained model\n",
    "***\n",
    "You can continue with the default model, or can choose a different model from the dropdown generated upon running the next cell. A complete list of SageMaker pre-trained models can also be accessed at [SageMaker pre-trained Models](https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html#).\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88228f3b",
   "metadata": {
    "jumpStartAlterations": [
     "modelIdVersion"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = \"huggingface-text2text-flan-t5-xl\", \"*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170e1228",
   "metadata": {},
   "source": [
    "***\n",
    "[Optional] Select a different SageMaker pre-trained model. Here, we download the model_manifest file from the Built-In Algorithms s3 bucket, filter-out all the Text Generation models and select a model for inference.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d8a1f7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ipywidgets import Dropdown\n",
    "from sagemaker.jumpstart.notebook_utils import list_jumpstart_models\n",
    "\n",
    "# Retrieves all Text Generation models available by SageMaker Built-In Algorithms.\n",
    "filter_value = \"task == text2text\"\n",
    "text_generation_models = list_jumpstart_models(filter=filter_value)\n",
    "\n",
    "# display the model-ids in a dropdown to select a model for inference.\n",
    "model_dropdown = Dropdown(\n",
    "    options=text_generation_models,\n",
    "    value=model_id,\n",
    "    description=\"Select a model\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout={\"width\": \"max-content\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28d45e5",
   "metadata": {},
   "source": [
    "#### Choose a model for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52b7a67a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c870a4227bb04dc9a21f79ad8cb3383a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(model_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "271642dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_version=\"*\" fetches the latest version of the model\n",
    "model_id, model_version = model_dropdown.value, \"1.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b08aa4a",
   "metadata": {},
   "source": [
    "### 3. Retrieve Artifacts & Deploy an Endpoint\n",
    "\n",
    "***\n",
    "\n",
    "Using SageMaker, we can perform inference on the pre-trained model, even without fine-tuning it first on a new dataset. We start by retrieving the `deploy_image_uri`, `deploy_source_uri`, and `model_uri` for the pre-trained model. To host the pre-trained model, we create an instance of [`sagemaker.model.Model`](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html) and deploy it. This may take a few minutes.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c122ee7e-4180-4f4b-8cc9-cf60d3e2b6f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sagemaker_session(local_download_dir) -> sagemaker.Session:\n",
    "    \"\"\"Return the SageMaker session.\"\"\"\n",
    "\n",
    "    sagemaker_client = boto3.client(\n",
    "        service_name=\"sagemaker\", region_name=boto3.Session().region_name\n",
    "    )\n",
    "\n",
    "    session_settings = sagemaker.session_settings.SessionSettings(\n",
    "        local_download_dir=local_download_dir\n",
    "    )\n",
    "\n",
    "    # the unit test will ensure you do not commit this change\n",
    "    session = sagemaker.session.Session(\n",
    "        sagemaker_client=sagemaker_client, settings=session_settings\n",
    "    )\n",
    "\n",
    "    return session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44051043-7405-457e-809f-a7c004646943",
   "metadata": {},
   "source": [
    "We need to create a directory to host the downloaded model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fcfc73e-00b1-4672-b327-99a477abfbc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p download_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06d1b55",
   "metadata": {},
   "source": [
    "---\n",
    "This text-to-text generation task supports a wide variety of model sizes that have different compute requirements. Here, we specify the instance type for several large models along with an environment variable to set the multi-model endpoint number of workers to 1. This ensures we can support the largest possible token lengths since additional models are not consuming GPU memory resources.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63611d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "_large_model_env = {\n",
    "    \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\",\n",
    "    \"TS_DEFAULT_WORKERS_PER_MODEL\": \"1\"\n",
    "}\n",
    "_model_env_variable_map = {\n",
    "    \"huggingface-text2text-flan-t5-xxl\": _large_model_env,\n",
    "    \"huggingface-text2text-flan-t5-xxl-fp16\": _large_model_env,\n",
    "    \"huggingface-text2text-flan-t5-xxl-bnb-int8\": _large_model_env,\n",
    "    \"huggingface-text2text-flan-t5-xl\": {\"MMS_DEFAULT_WORKERS_PER_MODEL\": \"1\"},\n",
    "    \"huggingface-text2text-flan-t5-large\": {\"MMS_DEFAULT_WORKERS_PER_MODEL\": \"1\"},\n",
    "    \"huggingface-text2text-flan-ul2-bf16\": _large_model_env,\n",
    "    \"huggingface-text2text-bigscience-t0pp\": _large_model_env,\n",
    "    \"huggingface-text2text-bigscience-t0pp-fp16\": _large_model_env,\n",
    "    \"huggingface-text2text-bigscience-t0pp-bnb-int8\": _large_model_env,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b61550d0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "---------!"
     ]
    }
   ],
   "source": [
    "from sagemaker import image_uris, instance_types, model_uris, script_uris\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "\n",
    "endpoint_name = name_from_base(f\"jumpstart-example-{model_id}\")\n",
    "\n",
    "# Retrieve the inference instance type for the specified model.\n",
    "instance_type = instance_types.retrieve_default(\n",
    "    model_id=model_id, model_version=model_version, scope=\"inference\"\n",
    ")\n",
    "\n",
    "# Retrieve the inference docker container uri. This is the base HuggingFace container image for the default model above.\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,  # automatically inferred from model_id\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=instance_type,\n",
    ")\n",
    "\n",
    "# Retrieve the inference script uri. This includes all dependencies and scripts for model loading, inference handling etc.\n",
    "deploy_source_uri = script_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, script_scope=\"inference\"\n",
    ")\n",
    "\n",
    "# Retrieve the model uri.\n",
    "model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"inference\"\n",
    ")\n",
    "\n",
    "# Create the SageMaker model instance\n",
    "if model_id in _model_env_variable_map:\n",
    "    # For those large models, we already repack the inference script and model\n",
    "    # artifacts for you, so the `source_dir` argument to Model is not required.\n",
    "    model = Model(\n",
    "        image_uri=deploy_image_uri,\n",
    "        model_data=model_uri,\n",
    "        role=aws_role,\n",
    "        predictor_cls=Predictor,\n",
    "        name=endpoint_name,\n",
    "        env=_model_env_variable_map[model_id],\n",
    "    )\n",
    "else:\n",
    "    model = Model(\n",
    "        image_uri=deploy_image_uri,\n",
    "        source_dir=deploy_source_uri,\n",
    "        model_data=model_uri,\n",
    "        entry_point=\"inference.py\",  # entry point file in source_dir and present in deploy_source_uri\n",
    "        role=aws_role,\n",
    "        predictor_cls=Predictor,\n",
    "        name=endpoint_name,\n",
    "        sagemaker_session=get_sagemaker_session(\"download_dir\"),\n",
    "    )\n",
    "\n",
    "# deploy the Model. Note that we need to pass Predictor class when we deploy model through Model class,\n",
    "# for being able to run inference through the sagemaker API.\n",
    "model_predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    predictor_cls=Predictor,\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9c254b",
   "metadata": {},
   "source": [
    "### 4. Query endpoint and parse response\n",
    "\n",
    "---\n",
    "Input to the endpoint is any string of text formatted as json and encoded in `utf-8` format. Output of the endpoint is a `json` with generated text.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "439998c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "newline, bold, unbold = \"\\n\", \"\\033[1m\", \"\\033[0m\"\n",
    "\n",
    "\n",
    "def query_endpoint(encoded_text, endpoint_name):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=\"application/x-text\", Body=encoded_text\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "def parse_response(query_response):\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    generated_text = model_predictions[\"generated_text\"]\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5d644d",
   "metadata": {},
   "source": [
    "---\n",
    "Below, we put in some example input text. You can put in any text and the model predicts next words in the sequence. Longer sequences of text can be generated by calling the model repeatedly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f82098b-5549-4e06-9636-7f10d33cc0ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference:\n",
      "input text: Translate to German:  My name is Arthur\n",
      "generated text: \u001b[1mIch bin Arthur.\u001b[0m\n",
      "\n",
      "Inference:\n",
      "input text: A step by step recipe to make bolognese pasta:\n",
      "generated text: \u001b[1mIn a large saucepan, combine the ground beef, onion, garlic, tomato paste, tomato\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newline, bold, unbold = \"\\n\", \"\\033[1m\", \"\\033[0m\"\n",
    "\n",
    "text1 = \"Translate to German:  My name is Arthur\"\n",
    "text2 = \"A step by step recipe to make bolognese pasta:\"\n",
    "\n",
    "\n",
    "for text in [text1, text2]:\n",
    "    query_response = query_endpoint(text.encode(\"utf-8\"), endpoint_name=endpoint_name)\n",
    "    generated_text = parse_response(query_response)\n",
    "    print(\n",
    "        f\"Inference:{newline}\"\n",
    "        f\"input text: {text}{newline}\"\n",
    "        f\"generated text: {bold}{generated_text}{unbold}{newline}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7fd94c8-6349-40ca-ae44-fedb4de3dc20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Place the pizza dough on a baking sheet. Spread the pizza sauce over the dough. Sprinkle the mozzarella cheese on top of the pizza sauce. Place the pizza on the baking sheet. Bake for 15 minutes at 450 degrees F.', 'Step 1: Preheat oven to 450 degrees. Step 2: Spread pizza sauce on pizza crust. Step 3: Top with cheese and pepperoni. Step 4: Bake at 450 degrees for 15 minutes.', 'To make a pizza, you first need to prepare the dough. You then spread the pizza sauce on the dough. You then place the toppings on the pizza. You then bake the pizza in a preheated oven.']\n"
     ]
    }
   ],
   "source": [
    "# Input must be a json\n",
    "payload = {\n",
    "    \"text_inputs\": \"Tell me the steps to make a pizza\",\n",
    "    \"max_length\": 50,\n",
    "    \"max_time\": 50,\n",
    "    \"num_return_sequences\": 3,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": True,\n",
    "}\n",
    "\n",
    "\n",
    "def query_endpoint_with_json_payload(encoded_json, endpoint_name):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=\"application/json\", Body=encoded_json\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "query_response = query_endpoint_with_json_payload(\n",
    "    json.dumps(payload).encode(\"utf-8\"), endpoint_name=endpoint_name\n",
    ")\n",
    "\n",
    "\n",
    "def parse_response_multiple_texts(query_response):\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    generated_text = model_predictions[\"generated_texts\"]\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "generated_texts = parse_response_multiple_texts(query_response)\n",
    "print(generated_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8262dfcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_return_sequences = 1\n",
    "parameters = {\n",
    "    \"max_length\": 50,\n",
    "    \"max_time\": 50,\n",
    "    \"num_return_sequences\": num_return_sequences,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": True,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a695b93c-8a22-4542-86ff-d2f2aaac6652",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_response(text,prompts):\n",
    "    newline, bold, unbold = \"\\n\", \"\\033[1m\", \"\\033[0m\"\n",
    "    \n",
    "\n",
    "    #print(f\"{bold}Number of return sequences are set as {num_return_sequences}{unbold}{newline}\")\n",
    "    for each_prompt in prompts:\n",
    "        payload = {\"text_inputs\": each_prompt.replace(\"{text}\", text), **parameters}\n",
    "        query_response = query_endpoint_with_json_payload(\n",
    "            json.dumps(payload).encode(\"utf-8\"), endpoint_name=endpoint_name\n",
    "        )\n",
    "        generated_texts = parse_response_multiple_texts(query_response)\n",
    "        #print(f\"{bold} For prompt: '{each_prompt}'{unbold}{newline}\")\n",
    "        #print(f\"{bold} The {num_return_sequences} summarized results are{unbold}:{newline}\")\n",
    "        for idx, each_generated_text in enumerate(generated_texts):\n",
    "            print(f\"{bold}Result {idx}{unbold}: {each_generated_text}{newline}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2522d5d-2723-40bd-95d5-fe2f4f5b5df1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 6. Zero-Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "02592c67-d1b1-41dd-8b70-2921babc8ede",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mResult 0\u001b[0m: neutral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"I think the vacation is okay.\"\n",
    "prompts = [\n",
    "    \"Classify the text into neutral, negative or positive.: {text} \\n Sentiment:\"\n",
    "]\n",
    "\n",
    "print_response(text,prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12217335-049d-4864-ac49-81d3466bc0d6",
   "metadata": {},
   "source": [
    "### 7. Few-Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4491245a-cf79-4408-8e7d-ca9a2f9d5fdb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A whatpu is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is: We were traveling in Africa and we saw these very cute whatpus.\n"
     ]
    }
   ],
   "source": [
    "object_in_sentence=\"whatpu\"\n",
    "text = f\"A {object_in_sentence} is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is: We were traveling in Africa and we saw these very cute whatpus.\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "73ab99d9-8be8-4c35-ad04-255e728d6fb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mResult 0\u001b[0m: She gave the ice cream man a farduddle.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "object_in_sentence=\"farduddle\"\n",
    "prompts = [\n",
    "    \"To do a {object_in_sentence} means to jump up and down really fast. An example of a sentence that uses the word farduddle is:\"\n",
    "]\n",
    "print_response(\"\",prompts)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c597294a-e5ce-441b-b670-46f1a3b755b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mResult 0\u001b[0m: Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"This is awesome! // Negative \\n\"\n",
    "    \"This is bad! // Positive \\n\"\n",
    "    \"Wow that movie was rad! // Positive \\n\"\n",
    "    \"What a horrible show! // \\n\"\n",
    "]\n",
    "print_response(\"\",prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b9e6fafe-1256-4966-89e8-25701b809349",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mResult 0\u001b[0m: Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Positive This is awesome! \\n\" \n",
    "    \"This is bad! Negative \\n\"\n",
    "    \"Wow that movie was rad! \\n\"\n",
    "    \"Positive \\n\"\n",
    "    \"What a horrible show! -- \\n\"\n",
    "]\n",
    "print_response(\"\",prompts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0129fd99-9945-4d12-b4f5-f754c54be545",
   "metadata": {},
   "source": [
    "### Limitations of Few-shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34004186-6cc4-4939-a023-ac844a1e2760",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_return_sequences = 1\n",
    "parameters = {\n",
    "    \"max_length\": 600,\n",
    "    \"num_return_sequences\": num_return_sequences,\n",
    "    \"top_p\": 0.01,\n",
    "    \"do_sample\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "015931a2-3d64-41e3-8c13-b509855cd9dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mResult 0\u001b[0m: a.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \\n\"\n",
    "prompts = [\n",
    "   \"A: \"\n",
    "]\n",
    "\n",
    "print_response(text,prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a85938a1-6b7f-4c77-afd3-a7cd1035f90d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mResult 0\u001b[0m: The answer is False.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\"\n",
    "prompts = [ \"\"\"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "    A: The answer is False.\n",
    "    ###\n",
    "    \n",
    "    The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
    "    A: The answer is True.\n",
    "    ###\n",
    "\n",
    "    The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
    "    A: The answer is True.\n",
    "    ###\n",
    "\n",
    "    The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
    "    A: The answer is False.\n",
    "    ###\n",
    "\n",
    "    {text}\n",
    "    A: \"\"\"\n",
    "]\n",
    "\n",
    "print_response(text,prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908d7861-538a-48b2-8cd0-f097fa140805",
   "metadata": {},
   "source": [
    "### 8. Chain of Thought Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdb7ce1-9df7-4b8d-a58f-d74cb01d4313",
   "metadata": {},
   "source": [
    "### Zero-shot COT Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d2d6e54e-d5b3-462d-ad56-b145d15ba2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mResult 0\u001b[0m: 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\n",
    "prompts = [ \"\"\"I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman.\\\n",
    "I then went and bought 5 more apples and ate 1. How many apples did I remain with? \"\"\"\n",
    "]\n",
    "\n",
    "print_response(text,prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d3475281-7142-4ae8-8606-7964c10e492d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mResult 0\u001b[0m: After giving 2 apples to the neighbor and 2 to the repairman, I remained with 10 - 2 - 2 = 6 apples. After buying 5 more apples, I remained with 6 + 5 = 11 apples. After eating 1 apple, I remained with 11 - 1 = 10 apples. The answer: 10.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\n",
    "prompts = [ \"\"\"I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman.\\\n",
    "I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
    "\n",
    "Let's think step by step. \"\"\"\n",
    "]\n",
    "\n",
    "print_response(text,prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a0be90-577b-48ca-abb9-45f432bb8510",
   "metadata": {},
   "source": [
    "### Automatic Chain of Thought(Auto-COT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "657d94b6-9620-4c10-ae6e-e5c9bb6a4236",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mResult 0\u001b[0m: Wendy uploaded 45 - 27 = 18 pictures into 9 different albums. Each album would have 2 pictures. The final answer: 2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\n",
    "prompts = [ \"\"\"Q: Wendy uploaded 45 pictures to Facebook. She put 27 pics into one album and put the rest into 9 different albums. How many pictures were in each album?\\nA:\n",
    "Let's think step by step. First, we know that Wendy uploaded 45 pictures in total. Second, we know that Wendy put 27 pictures into one album. That means that Wendy put the remaining 18 pictures into 9 different albums. That means that each album would have 2 pictures. \"\"\"\n",
    "]\n",
    "print_response(text,prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eb41f3-3578-48d3-a783-a48fb6ef4846",
   "metadata": {},
   "source": [
    "More auto-cot examples can be tested at https://github.com/amazon-science/auto-cot/blob/main/try_cot.ipynb (works better on an instance(e.g g4dn.2xlarge) with torch and sentence-transformers installed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f0e6d1-e533-4cc6-a7d9-b90027d3d2f5",
   "metadata": {},
   "source": [
    "### Self Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b062a912-7c6b-4b93-9c68-6a99b762478e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mResult 0\u001b[0m: 60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\n",
    "prompts = [ \"\"\" When I was 6 my sister was half my age. Now I’m 70 how old is my sister? \"\"\"]\n",
    "print_response(text,prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5339fc49-f990-4e9d-98b4-648e08c0b367",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mResult 0\u001b[0m: Let x be the age of my sister when I was 6 years old. Now, x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x + x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\n",
    "prompts = [ \"\"\" Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done,\n",
    "there will be 21 trees. How many trees did the grove workers plant today? \n",
    "A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted.\n",
    "So, they must have planted 21 - 15 = 6 trees. The answer is 6.\n",
    "\n",
    "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\n",
    "\n",
    "Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
    "A: Leah had 32 chocolates and Leah’s sister had 42. That means there were originally 32 + 42 = 74\n",
    "chocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.\n",
    "\n",
    "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops\n",
    "did Jason give to Denny?\n",
    "A: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of\n",
    "lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.\n",
    "\n",
    "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does\n",
    "he have now?\n",
    "A: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so\n",
    "in total he has 7 + 2 = 9 toys. The answer is 9.\n",
    "\n",
    "Q: There were nine computers in the server room. Five more computers were installed each day, from\n",
    "monday to thursday. How many computers are now in the server room?\n",
    "A: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 =\n",
    "20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers.\n",
    "The answer is 29.\n",
    "\n",
    "Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many\n",
    "golf balls did he have at the end of wednesday?\n",
    "A: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On\n",
    "Wednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.\n",
    "\n",
    "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
    "A: She bought 5 bagels for $3 each. This means she spent $15. She has $8 left.\n",
    "\n",
    "Q: When I was 6 my sister was half my age. Now I’m 70 how old is my sister?\n",
    "A: \"\"\"]\n",
    "print_response(text,prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5c675a78-f007-4eec-bfbd-c55a72703130",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mResult 0\u001b[0m: Let x be the age of my sister when I was 6 years old. Now, x + x + 6 = 70. x = 70 / 2 = 35. x = 35 years old. The answer is 35.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\" \"\"\"\n",
    "prompts = [ \"\"\" Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done,\n",
    "there will be 21 trees. How many trees did the grove workers plant today? \n",
    "A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted.\n",
    "So, they must have planted 21 - 15 = 6 trees. The answer is 6.\n",
    "\n",
    "Q: When I was 6 my sister was half my age. Now I’m 70 how old is my sister?\n",
    "A: \"\"\"]\n",
    "print_response(text,prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f9babef5-4713-48c0-8ed7-acbf6e875e06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_return_sequences = 3\n",
    "parameters = {\n",
    "    \"max_length\": 500,\n",
    "    \"max_time\": 50,\n",
    "    \"num_return_sequences\": num_return_sequences,\n",
    "    \"top_p\": 0.7,\n",
    "    \"do_sample\": False,\n",
    "    \"temperature\":0.7\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bc417bd3-b19b-4316-b385-a1825b7a96fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mResult 0\u001b[0m: My sister was 6 / 2 = 3 years old when I was 6. So she’s 70 - 6 = 56 years old now. The answer is 56.\n",
      "\n",
      "\u001b[1mResult 1\u001b[0m: My sister was 6 / 2 = 3 years old when I was 6. So she’s 70 - 6 = 62 years old now. The answer is 62.\n",
      "\n",
      "\u001b[1mResult 2\u001b[0m: My sister was 6 / 2 = 3 years old when I was 6. So she’s 70 - 6 = 56 years old. The answer is 56.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\n",
    "prompts = [ \"\"\" Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done,\n",
    "there will be 21 trees. How many trees did the grove workers plant today? \n",
    "A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted.\n",
    "So, they must have planted 21 - 15 = 6 trees. The answer is 6.\n",
    "###\n",
    "\n",
    "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\n",
    "###\n",
    "\n",
    "Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
    "A: Leah had 32 chocolates and Leah’s sister had 42. That means there were originally 32 + 42 = 74\n",
    "chocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.\n",
    "###\n",
    "\n",
    "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops\n",
    "did Jason give to Denny?\n",
    "A: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of\n",
    "lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.\n",
    "###\n",
    "\n",
    "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does\n",
    "he have now?\n",
    "A: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so\n",
    "in total he has 7 + 2 = 9 toys. The answer is 9.\n",
    "###\n",
    "\n",
    "Q: There were nine computers in the server room. Five more computers were installed each day, from\n",
    "monday to thursday. How many computers are now in the server room?\n",
    "A: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 =\n",
    "20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers.\n",
    "The answer is 29.\n",
    "###\n",
    "\n",
    "Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many\n",
    "golf balls did he have at the end of wednesday?\n",
    "A: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On\n",
    "Wednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.\n",
    "###\n",
    "\n",
    "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
    "A: She bought 5 bagels for $3 each. This means she spent $15. She has $8 left.\n",
    "###\n",
    "\n",
    "Q: When I was 6 my sister was half my age. Now I’m 70. how old is my sister?\n",
    "A: \"\"\"]\n",
    "print_response(text,prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "caf1cee2-566e-4252-906a-39d1fb361f94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mResult 0\u001b[0m: No\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_return_sequences = 1\n",
    "parameters = {\n",
    "    \"max_length\": 500,\n",
    "    \"max_time\": 50,\n",
    "    \"num_return_sequences\": num_return_sequences,\n",
    "    \"top_p\": 0.7,\n",
    "    \"do_sample\": False,\n",
    "    \"temperature\":0.7\n",
    "}\n",
    "text = \"\"\n",
    "prompts = [ \"\"\" Part of golf is trying to get a higher point total than others. Yes or No? \"\"\"]\n",
    "print_response(text,prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "23f22a83-d7da-4deb-90e4-19afdd38e674",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mResult 0\u001b[0m: Golf is a game in which players try to get a higher score than other players. The higher the score, the higher the point total is for the player who scored the highest number of points in a round of golf. The higher the score, the higher the point total is for the player who scored the lowest number of points in a round of golf. The higher the score, the lower the point total is for the player who scored the lowest number of points in a round of golf.\n",
      "\n",
      "\u001b[1mResult 1\u001b[0m: Golf is a game in which players try to get a higher score than other players. The higher the score, the higher the point total is for the player who scored the highest number of points in a round of golf. The higher the score, the higher the point total is for the player who scored the lowest number of points in a round of golf. The higher the score, the lower the point total is for the player who scored the highest number of points in a round of golf.\n",
      "\n",
      "\u001b[1mResult 2\u001b[0m: Golf is a game in which players try to get a higher score than other players. The higher the score, the higher the point total is for the player who scored the highest number of points in a round of golf. The higher the score, the higher the point total is for the player who scored the lowest number of points in a round of golf. The higher the score, the lower the point total is for the player who scored the second lowest number of points in a round of golf.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_return_sequences = 3\n",
    "parameters = {\n",
    "    \"max_length\": 400,\n",
    "    \"min_length\": 100,\n",
    "    \"max_time\": 50,\n",
    "    \"num_return_sequences\": num_return_sequences,\n",
    "    \"top_p\": 0.4,\n",
    "    \"do_sample\": False,\n",
    "    \"temperature\":0.3\n",
    "}\n",
    "text = \"\"\n",
    "prompts = [ \"\"\" Input: Greece is larger than mexico.\n",
    "Knowledge: Greece is approximately 131,957 sq km, while Mexico is approximately 1,964,375 sq km, making Mexico 1,389% larger than Greece.\n",
    "\n",
    "Input: Glasses always fog up.\n",
    "Knowledge: Condensation occurs on eyeglass lenses when water vapor from your sweat, breath, and ambient humidity lands on a cold surface, cools, and then changes into tiny drops of liquid, forming a film that you see as fog. Your lenses will be relatively cool compared to your breath, especially when the outside air is cold.\n",
    "\n",
    "Input: A fish is capable of thinking.\n",
    "Knowledge: Fish are more intelligent than they appear. In many areas, such as memory, their cognitive powers match or exceed those of ’higher’ vertebrates including non-human primates. Fish’s long-term memories help them keep track of complex social relationships.\n",
    "\n",
    "Input: A common effect of smoking lots of cigarettes in one’s lifetime is a higher than normal chance of getting lung cancer.\n",
    "Knowledge: Those who consistently averaged less than one cigarette per day over their lifetime had nine times the risk of dying from lung cancer than never smokers. Among people who smoked between one and 10 cigarettes per day, the risk of dying from lung cancer was nearly 12 times higher than that of never smokers.\n",
    "\n",
    "Input: A rock is the same size as a pebble.\n",
    "Knowledge: A pebble is a clast of rock with a particle size of 4 to 64 millimetres based on the Udden-Wentworth scale of sedimentology. Pebbles are generally considered larger than granules (2 to 4 millimetres diameter) and smaller than cobbles (64 to 256 millimetres diameter).\n",
    "\n",
    "Input: Part of golf is trying to get a higher point total than others.\n",
    "Knowledge: \"\"\"]\n",
    "print_response(text,prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c1016580-e0b4-4bd2-b2f2-9eef9af7b2b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mResult 0\u001b[0m: Yes - Golf is a game in which players try to get a higher point total than others. The higher the score, the higher the point total is for the player who scored the highest number of points in a round of golf. The higher the score, the higher the point total is for the player who scored the lowest number of points in a round of golf. The higher the score, the lower the point total is for the player who scored the lowest number of points in a round of golf.\n",
      "\n",
      "\u001b[1mResult 1\u001b[0m: Yes - Golf is a game in which players try to get a higher point total than others. The higher the score, the higher the point total is for the player who scored the highest number of points in a round of golf. The higher the score, the higher the point total is for the player who scored the lowest number of points in a round of golf. The higher the score, the lower the point total is for the player who scored the lowest number of points in a round of golf. Yes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_return_sequences = 2\n",
    "parameters = {\n",
    "    \"max_length\": 400,\n",
    "    \"min_length\": 100,\n",
    "    \"max_time\": 50,\n",
    "    \"num_return_sequences\": num_return_sequences,\n",
    "    \"top_p\": 0.4,\n",
    "    \"do_sample\": False,\n",
    "    \"temperature\":0.3\n",
    "}\n",
    "text = \"\"\n",
    "prompts = [ \"\"\" Question: Part of golf is trying to get a higher point total than others. Yes or No?\n",
    "Knowledge: Golf is a game in which players try to get a higher score than other players. The higher the score, the higher the point total is for the player who scored the highest number of points in a round of golf. The higher the score, the higher the point total is for the player who scored the lowest number of points in a round of golf. The higher the score, the lower the point total is for the player who scored the lowest number of points in a round of golf.\n",
    "###\n",
    "\n",
    "Explain and Answer:  \"\"\"]\n",
    "print_response(text,prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cf58ce-49c6-49a3-ac5c-f8902a11381a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tree of thoughts\n",
    "The ToT technique is inspired by the human mind’s approach for solving complex reasoning tasks through trial and error. In this process, the human mind explores the solution space through a tree-like thought process, allowing for backtracking when necessary. To implement ToT as a software system, we augment an LLM with additional modules including a prompter agent, a checker module, a memory module, and a ToT controller. In order to solve a given problem, these modules engage in a multi-round conversation with the LLM. Unlike an auto-regressive LLM which generates a new token based on the preceding sequence of tokens without backward editing, the ToT framework allows the sytem to backtrack to the previous steps of the thought-process and explore other directions from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "786d9abc-2897-44f9-a055-875d24a64a06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mResult 0\u001b[0m: No Expert: Golf is a game of skill, not a game of points. Expert: No Expert: Golf is a game of skill, not a game of points. Expert: No Expert: Golf is a game of skill, not a game of points. Expert: No Expert: Golf is a game of skill, not a game of points. Expert: No Expert: Golf is a game of skill, not a game of points. Expert: No Expert: Golf is a game of skill, not a game of points. Expert: No Expert: Golf is a game of skill, not a game of points. Expert: No Expert: Golf is a game of skill, not a game of points. Expert: No Expert: Golf is a game of skill, not a game of points. Expert: No Expert: Golf is a game of skill, not a game of points. Expert: No Expert: Golf is a game of skill, not a game of points. Expert: No Expert: Golf is a game of skill, not a game of points. Expert: No Expert: Golf is a game of skill, not a game of points. Expert: No Expert: Golf is a game of skill, not a game of points. Expert: No Expert: Golf is a game of skill, not a game of points. Expert: No Expert: Golf is a game of skill, not a game of points. Expert: No Expert: Golf is a game of skill, not a game of points.\n",
      "\n",
      "\u001b[1mResult 1\u001b[0m: No Expert: Golf is a game of skill, not a game of points. Expert: No Expert: Golf is a game of skill, not a game of points. Expert: No Expert: Golf is a game of skill, not a game of points. Expert: No Expert: Golf is a game of skill, not a game of points. Expert: No Expert: Golf is a game of skill, not a game of points. Expert: No Expert: Golf is a game of skill, not a game of points. Expert: No Expert: Golf is a game of skill, not a game of points. Expert: No Expert: Golf is a game of skill, not a game of points. Expert: No Expert: Golf is a game of skill, not a game of points. Expert: No Expert: Golf is a game of skill, not a game of points. Expert: No Expert: Golf is a game of skill, not a game of points. Expert: No Expert: Golf is a game of skill, not a game of points. Expert: No Expert: Golf is a game of skill, not a game of points. Expert: No Expert: Golf is a game of skill, not a game of points. Expert: No Expert: Golf is a game of skill, not a game of points. Expert: No Expert: Golf is a game of skill, not a game of points.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_return_sequences = 2\n",
    "parameters = {\n",
    "    \"max_length\": 400,\n",
    "    \"min_length\": 100,\n",
    "    \"max_time\": 50,\n",
    "    \"num_return_sequences\": num_return_sequences,\n",
    "    \"top_p\": 0.4,\n",
    "    \"do_sample\": False,\n",
    "    \"temperature\":0.3\n",
    "}\n",
    "text = \"\"\n",
    "prompts = [ \"\"\" Imagine three different experts are answering this question.\n",
    "All experts will write down 1 step of their thinking,\n",
    "then share it with the group.\n",
    "Then all experts will go on to the next step, etc.\n",
    "If any expert realises they're wrong at any point then they leave.\n",
    "The question is: Part of golf is trying to get a higher point total than others. Yes or No?\"\"\"]\n",
    "print_response(text,prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeeda48-f6aa-41ad-b11d-6495e22d3e78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa5de21f",
   "metadata": {},
   "source": [
    "### 7. Clean up the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69b588d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_predictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Delete the SageMaker endpoint\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel_predictor\u001b[49m\u001b[38;5;241m.\u001b[39mdelete_model()\n\u001b[1;32m      3\u001b[0m model_predictor\u001b[38;5;241m.\u001b[39mdelete_endpoint()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_predictor' is not defined"
     ]
    }
   ],
   "source": [
    "# Delete the SageMaker endpoint\n",
    "model_predictor.delete_model()\n",
    "model_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3c4493-c6db-4065-8f78-fdaf6ff07e78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
